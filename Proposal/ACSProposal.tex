\documentclass[11pt]{article}
\usepackage{a4wide,parskip,times}
\usepackage{hyperref}

\begin{document}

\centerline{\Large A tool for covertness benchmarking of Tor pluggable transports}
\vspace{2em}
\centerline{\Large \emph{An MPhil project proposal}}
\vspace{2em}
\centerline{\large Chongyang Shi (\emph{cs940}), Christ's College}
\vspace{1em}
\centerline{\large Project Supervisor: TBC}
\vspace{1em}

\begin{abstract}
\textsl{Censorship-circumventing Tor network traffic can be obfuscated as random traffic or traffic of a different protocol through the use of pluggable transport (PT) protocols. Past research efforts in means to detect obfuscated PT traffic have yielded various traffic analysis techniques, with varying performance and target protocol suitabilities. Inspired by related research on attacking image-watermarking systems, this proposed project intends to develop a benchmarking tool for evaluating the covertness of PT protocols through combinations of analysis techniques under current research. A baseline covertness against traffic classification could be established for development of new PT protocols.} 
\end{abstract}

\section{Introduction, approach and outcomes}

Tor is a popular tool for anonymised and censorship-resistant network communications. While it is trivial for a network node in a privileged position to detect and block non-obfuscated Tor traffic \cite[Tb. 6] {bujlow2015independent} in a process called \emph{traffic classification}, Tor provides a set of \emph{pluggable transport} (PT) tools which clients can use to conceal their connections to a Tor bridge node from such censors. An arms race between state-sponsored censors and PT developers in traffic obfuscation has been going on for many years \cite{khattak2014systemization}. 

Among pluggable transports, two classes of techniques currently exist to achieve obfuscation of the encrypted traffic: pseudo-random transformation and mimicry of other ``legitimate'' protocols. Techniques in the former class attempt to avoid traffic classification by transforming Tor traffic into pseudo-random data, while those in the latter class transform Tor traffic into the likes of various other protocols that will result in too much collateral damage for the censor to block. A number of tools have been developed in each class and deployed with Tor distributions, with pseudo-random transformation represented by ScrambleSuit \cite{winter2013scramblesuit} and Format-Transforming Encryption (FTE) \cite{dyer2013protocol}, and mimicry represented by meek \cite{fifield2015blocking} and SkypeMorph \cite{mohajeri2012skypemorph}. 

Obfuscation techniques can generally be evaluated on two metrics: the transmission performance after obfuscation, and the covertness of obfuscated traffic in regular traffic when examined by a state censor. For the purpose of censorship-circumvention, interests are usually concentrated on the latter. There has been a few independent covertness evaluations on the aforementioned tools \cite{tan2015towards} \cite{houmansadr2013parrot} \cite{wang2015seeing} over recent years. This is however still a relatively niche field of research when compared with related fields such as cipher cryptanalysis and steganography, whose methodologies and techniques could be adapted into use in this field.

Under current research efforts, three categories of attack techniques are used to detect obfuscated traffic: semantics-based attacks where behaviour of traffic is checked against expected behaviours of its protocol \cite[Sec. VIII]{houmansadr2013parrot} \cite[Sec. 4]{wang2015seeing}; entropy-based attacks where entropy signatures of packet payloads can be established for regular and obfuscated traffic \cite{tan2015towards} \cite[Sec. 5]{wang2015seeing}; and machine learning-based attacks that can be effective against protocols resistant to two previous categories of attacks \cite[Sec. 6]{wang2015seeing}, but with significant drawbacks in portability between network environments \cite{dixon2016network}. 

All categories of attacks observe features in traffic traces such as packet metadata and distribution. Each category of technique has distinct superiorities and weaknesses in terms of computational cost, protocol coverage, and portability. They share the same the goal of achieving a high true-positive rate (identifying obfuscated traffic traces) and a low false-positive rate (not misidentifying non-obfuscated traffic as obfuscated), both of which are desirable to a state censor. 

Therefore, the primary objective of this proposed project is to produce a comprehensive benchmarking tool encompassing adapted versions of the aforementioned detection techniques. The tool will be able to accept sample traffic traces of any PT suitable for use with Tor, and perform varying combinations of traffic analysis techniques to evaluate the covertness of the PT protocol (perhaps with obfuscated-likelihood scoring from different techniques). Combinations of techniques can be chosen with deliberate strategies to maximise detection performance, as observed by Wang et al. \cite[Sec. 5.2]{wang2015seeing}. There is the possibility of automating the selection process. With varying thresholds on acceptable true-positive and false-negative rates, it would be possible to estimate whether a PT protocol is of required covertness standards, as performed by StirMark \cite{petitcolas1998attacks} on image-watermarking systems.

Individual detection techniques covered by past research efforts will be studied during integration into the benchmarking tool. As design and implementation details of PT protocols may have changed during the past few years, traffic traces from PT protocols will be examined again to verify previous observations on their identifying features, and attempts to find new features will be made.

Traffic traces used in this proposed study could be sourced from anonymised internet traces available for research purposes, injected with self-generated PT traffic traces, as conducted by Wang \emph{et. al.} \cite[Sec. 3]{wang2015seeing}. Alternatively, for traffic traces more resembling real network conditions, human volunteers can be invited to browse the internet in a monitored environment where Tor clients with PT are in use on some of the network clients, subject to ethical review approval.


\section{Workplan (500 words)}

Consulting reference implementations of some detection techniques by Wang et. al. \cite{wang2015seeing} \footnote{Implementations by Wang et. al.: https://github.com/liangw89/obfs-detection}, there is a reasonable level of confidence that individual detection techniques can effectively detect obfuscated traffic to some degree, despite the fact that Tor and pluggable transports have evolved since the publication of their work. In the process of developing a benchmarking tool encompassing these techniques, along with reimplementing individual techniques, the project will be focused on adapting and improving these techniques, so that they could be easily applied to arbitrary traffic traces without a lengthy parameter adjustment process (especially in the case of machine learning methods, to avoid overfitting). 

Another significant implementation challenge is obtaining the traffic traces required for testing the benchmarking tool. In general, two sets of network traffic traces are required: a large set of regular user traffic captured from regular internet browsing without the use of Tor and PT (required for false-positive detection test), and a smaller set of obfuscated PT traffic (consists of traces by different PT protocols for true-positive detection test). While it is possible to obtain anonymised real traffic traces from the Center for Applied Internet Data Analysis (CAIDA) \footnote{CAIDA: https://www.caida.org/data/}, application-layer contents have been stripped in the anonymisation process for obvious privacy reasons. Synthetic traffic generated by automated browsing can also result in significant flaws in detection strategy, as observed by Wang et. al. \cite[Sec. 6.1]{wang2015seeing}. Therefore the only option is to capture and store full traces of real world traffic produced specifically for experimental purposes.

As discussed in the earlier section, ethical review approval will be required for volunteers to be invited for creating real user traffic at scale. Volunteers will be informed that their internet traffic will be recorded and stored securely for the purpose of the proposed project only. They will also be asked be asked to not log into internet accounts or accessing personal information during the process. Only statistical data will be reported in the project report. Prior to approval for other volunteers to be brought into this process, the project student will be able to create smaller scales of both sets of traffic by capturing his own internet usage, allowing preliminary work to be conducted.

With these considerations in mind, the proposed work plan for the project is as followed, divided into two-week chunks within a 28-week project period:

\begin{itemize}
	\item \textbf{During Michaelmas Term}: Submit a request for departmental ethical review on the proposed volunteer traffic generation process, along with other project documents.
	\item \textbf{Chunk 1}: Study the reference implementation and select a suitable programming language for implementing the benchmarking tool. Preliminarily determine a strategy for combined detection.
	\item \textbf{Chunk 2}: Create a mechanism for reliably capturing network traffic traces with Wireshark, Tor (with PT), and potentially over VPN to a capture server. Test the mechanism by capturing own network traffic.
	\item \textbf{Chunk 3}: Study traffic traces of each PT to verify observations by past research efforts and attempt to identify new identifying features.
	\item \textbf{Chunks 4-6}: Implement and test individual techniques on their own, with traffic traces from own network usage. Details of detection techniques may be changed from existing research based on findings from tests.  If ethical review approves volunteer traffic generation, advertise with incentives for volunteers.
	\item \textbf{Chunks 7-8}: Conduct the volunteer traffic generation during this work segment. Implement and test the combined detection strategy with both sets of traffic traces. 
	\item \textbf{Chunks 9-10}: Evaluate combined detection performance and make adjustments to combined and individual techniques as necessary.
	\item \textbf{Chunks 11-13}: Project report write up (the literature review will likely have been written during the previous chunks).
	\item \textbf{Chunk 14}: Reserved for contingencies.
\end{itemize}

Work may be conducted quicker than planned if no significant problems occur and time permits, in which case additional detection methods may be explored, especially in the case of methods based on machine learning.

\bibliographystyle{IEEEtran}
\footnotesize{\bibliography{proposal}}


\end{document}
